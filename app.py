import streamlit as st
import google.generativeai as genai
from PIL import Image
import PyPDF2
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from streamlit_mic_recorder import mic_recorder

# -------------------- í˜ì´ì§€ ì„¤ì • --------------------
st.set_page_config(
    page_title="AI ë„ìš°ë¯¸",
    page_icon="ğŸš€",
    layout="wide"
)

# -------------------- í•¨ìˆ˜ ì •ì˜ --------------------
def extract_text_from_pdf(pdf_file):
    reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text

def split_text_into_chunks(text, chunk_size=1000, overlap=100):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunks.append(" ".join(words[i:i + chunk_size]))
    return chunks

# -------------------- ì‚¬ì´ë“œë°” (ì„¤ì •) --------------------
with st.sidebar:
    st.header("1. ê¸°ë³¸ ì„¤ì •")
    
    # [ìˆ˜ì •ëœ ë¶€ë¶„] Streamlit Cloudì˜ Secretsì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # ì´ ë¶€ë¶„ì€ ë°°í¬ ì‹œì—ë§Œ ì •ìƒ ì‘ë™í•˜ë©°, ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.
    if 'api_key' not in st.session_state:
        try:
            st.session_state.api_key = st.secrets["GOOGLE_API_KEY"]
            genai.configure(api_key=st.session_state.api_key)
            st.success("API í‚¤ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        except (KeyError, Exception):
            # ë¡œì»¬ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê²½ê³  ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ê±°ë‚˜, ì§ì ‘ í‚¤ë¥¼ ì…ë ¥í•˜ëŠ” ë¡œì§ì„ ì„ì‹œë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            st.warning("ë°°í¬ í™˜ê²½ì´ ì•„ë‹ˆê±°ë‚˜ Secretsì— API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.divider()
    st.header("2. ì±—ë´‡ í˜ë¥´ì†Œë‚˜ ì„¤ì •")
    system_prompt = st.text_area("AI ë¹„ì„œì˜ ì—­í• ì„ ì§€ì •í•´ì£¼ì„¸ìš”.", "ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.")

    st.divider()
    st.header("3. íŒŒì¼ í•™ìŠµ (RAG)")
    knowledge_file = st.file_uploader("ì§€ì‹ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©í•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")

    if st.button("íŒŒì¼ í•™ìŠµ ì‹œì‘"):
        # API í‚¤ê°€ ì„¸ì…˜ ìƒíƒœì— ìˆëŠ”ì§€ í™•ì¸
        if 'api_key' in st.session_state and st.session_state.api_key:
            if knowledge_file:
                with st.spinner("íŒŒì¼ì„ ì½ê³  í•™ìŠµí•˜ëŠ” ì¤‘..."):
                    try:
                        raw_text = extract_text_from_pdf(knowledge_file)
                        text_chunks = split_text_into_chunks(raw_text)
                        
                        embedding_model = 'models/text-embedding-004'
                        result = genai.embed_content(model=embedding_model, content=text_chunks, task_type="RETRIEVAL_DOCUMENT")
                        st.session_state.rag_embeddings = np.array(result['embedding'])
                        st.session_state.rag_chunks = text_chunks
                        st.success(f"{len(text_chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì¡°ê° í•™ìŠµ ì™„ë£Œ!")
                    except Exception as e:
                        st.error(f"íŒŒì¼ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            else:
                st.warning("í•™ìŠµí•  PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        else:
            st.warning("API í‚¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# -------------------- ë©”ì¸ í™”ë©´ --------------------
st.title("ğŸš€ AI ë„ìš°ë¯¸")
st.caption("ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸í•˜ê±°ë‚˜, í•™ìŠµëœ íŒŒì¼ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

# ëª¨ë¸ ë° ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
try:
    if 'api_key' in st.session_state and st.session_state.api_key:
        if "model" not in st.session_state or st.session_state.system_prompt != system_prompt:
            st.session_state.system_prompt = system_prompt
            st.session_state.model = genai.GenerativeModel('gemini-2.5-flash', system_instruction=system_prompt)
            st.session_state.messages = []
except Exception as e:
    st.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ìŒì„±, í…ìŠ¤íŠ¸) ---
prompt = st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")
audio_prompt = None

st.write("ë˜ëŠ”, ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”:")
audio_data = mic_recorder(start_prompt="ğŸ¤", stop_prompt="â¹ï¸", key='recorder')
if audio_data:
    if 'api_key' in st.session_state and st.session_state.api_key:
        with st.spinner("ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘..."):
            try:
                audio_path = "temp_audio.wav"
                with open(audio_path, "wb") as f:
                    f.write(audio_data['bytes'])
                
                audio_file = genai.upload_file(path=audio_path)
                response = genai.GenerativeModel('gemini-2.5-flash').generate_content(
                    ["ì´ ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì ì–´ì¤˜.", audio_file]
                )
                audio_prompt = response.text
                st.info(f"ìŒì„± ì¸ì‹ ê²°ê³¼: {audio_prompt}")
            except Exception as e:
                st.error(f"ìŒì„± ë³€í™˜ ì˜¤ë¥˜: {e}")
    else:
        st.warning("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ìŒì„± ì¸ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

final_prompt = prompt or audio_prompt

if final_prompt:
    if 'model' not in st.session_state:
        st.error("API í‚¤ë¥¼ ë¨¼ì € ì„¤ì •í•˜ê³  ëª¨ë¸ì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": final_prompt})
    with st.chat_message("user"):
        st.markdown(final_prompt)
        
    with st.chat_message("assistant"):
        with st.spinner("AIê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                augmented_prompt = final_prompt
                if "rag_embeddings" in st.session_state:
                    embedding_model = 'models/text-embedding-004'
                    query_embedding = genai.embed_content(model=embedding_model, content=final_prompt, task_type="RETRIEVAL_QUERY")['embedding']
                    
                    similarities = cosine_similarity(np.array(query_embedding).reshape(1, -1), st.session_state.rag_embeddings).flatten()
                    top_indices = similarities.argsort()[-3:][::-1]
                    
                    context = "\n---\n".join([st.session_state.rag_chunks[i] for i in top_indices])
                    augmented_prompt = f"ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´ì¤˜.\n\n[ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n[ì§ˆë¬¸]\n{final_prompt}"
                    st.info("â„¹ï¸ í•™ìŠµëœ íŒŒì¼ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.")

                response = st.session_state.model.generate_content(augmented_prompt)
                assistant_response = response.text
                st.markdown(assistant_response)
                st.session_state.messages.append({"role": "assistant", "content": assistant_response})
            except Exception as e:
                st.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
